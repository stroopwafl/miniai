{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6fb59a-06a9-408f-9bdd-a8d4904b3642",
   "metadata": {},
   "source": [
    "# Learner\n",
    "\n",
    "This notebook contains the source code for the Learner module in the MiniAI framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8634ad54-cb80-4431-b6fa-8dfabd91a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deb3485-9c0c-4414-8751-468dd9db12fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976d8bee-5feb-4dbb-a49e-c6af5f6f3a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math, torch, matplotlib.pyplot as plt, numpy as np\n",
    "from operator import itemgetter\n",
    "import fastcore.all as fc\n",
    "\n",
    "from torch import tensor, nn, optim\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset, load_dataset_builder\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "from fastcore.test import test_close\n",
    "from contextlib import contextmanager\n",
    "from functools import partial\n",
    "\n",
    "from operator import attrgetter,itemgetter\n",
    "from torcheval.metrics import *\n",
    "\n",
    "import pandas as pd\n",
    "from miniai.conv import *\n",
    "from miniai.datasets import *\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b486cc-e44b-46e5-ae89-d18f6c02ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c18070-2688-45d3-bde7-9b6d5f593f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "name = 'fashion_mnist'\n",
    "dsd = load_dataset(name)\n",
    "dsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e9bb4-a6ba-4c94-95ec-c78ef4775f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "@inplace\n",
    "def transform(b): b['image'] = [TF.to_tensor(i) for i in b['image']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645d92df-04f1-4fb2-9ffd-554ca42a983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "tds = dsd.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca41876-d85b-41a7-adc9-f501b8bb37fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "dls = DataLoaders.from_dd(tds, batch_size=256)\n",
    "b = next(iter(dls.train))\n",
    "xb, yb = b\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbb9e6e-3b9c-4dc7-803c-da5ab0e112bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def get_model():\n",
    "    return nn.Sequential(\n",
    "        ConvNormAct(1, 4),                 #14x14\n",
    "        ConvNormAct(4, 8),                 #7x7\n",
    "        ConvNormAct(8, 16),                #4x4\n",
    "        ConvNormAct(16, 32),               #2x2\n",
    "        ConvNormAct(32, 64),               #1x1\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(64, 10),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1b62aa-a561-4650-b806-f463ec297536",
   "metadata": {},
   "source": [
    "## Exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1153e175-4095-4ec4-86f6-a3da310058c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CancelFitException(Exception): pass\n",
    "class CancelBatchException(Exception): pass\n",
    "class CancelEpochException(Exception): pass\n",
    "class CancelFull_EpochException(Exception): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15501cee-a3e1-4bde-a053-bae7f8b40dfa",
   "metadata": {},
   "source": [
    "## Learner class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1bc15a-d2dc-404a-84ba-d1f195f19d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from torch.optim.lr_scheduler import OneCycleLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bdc2c8-e925-49e5-9799-1eeb6c2cab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Learner:\n",
    "    \"\"\"\n",
    "        Main flexible learner class that enables modular functionality to be added on.\n",
    "        It does so with a context manager, which wraps function calls with 'before' and \n",
    "        'after' callbacks, within which functionality can be added.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        dls, # Dataloaders object, expected as a tuple of (train, valid)\n",
    "        model, # Model used for training\n",
    "        opt_func=optim.SGD, # Optimisation function for optimising parameters after backprop, defaults to SGD\n",
    "        loss_func=F.cross_entropy, # Loss function used, defaults to cross entropy\n",
    "        cbs: list=None # Optional list of callback functions called via context manager\n",
    "    ):\n",
    "        fc.store_attr()\n",
    "        if cbs is not None:\n",
    "            for cb in cbs: cb.learn = self\n",
    "        \n",
    "    @contextmanager\n",
    "    def callback_context(self, name):\n",
    "        try:\n",
    "            self.callback(f\"before_{name}\")\n",
    "            yield\n",
    "            self.callback(f\"after_{name}\")\n",
    "        except globals()[f'Cancel{name.title()}Exception']: pass\n",
    "        \n",
    "    def fit(self, lr, epochs, one_cycle=False):\n",
    "        self.lr, self.n_epochs, self.epochs, self.one_cycle  = lr, epochs, range(epochs), one_cycle\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        with self.callback_context('fit'):\n",
    "            for self.epoch in self.epochs:\n",
    "                with self.callback_context('full_epoch'):\n",
    "                    self._one_epoch(train=True)\n",
    "                    self._one_epoch(train=False)\n",
    "        \n",
    "    def _one_epoch(self, train):\n",
    "        self.model.training = train\n",
    "        if train: self.dl = self.dls.train\n",
    "        else: self.dl = self.dls.valid\n",
    "        if self.one_cycle: self.scheduler = OneCycleLR(self.opt, max_lr=self.lr*1.3, total_steps=len(self.dl))\n",
    "        with self.callback_context('epoch'):\n",
    "            for self.batch in self.dl:\n",
    "                with self.callback_context('batch'):\n",
    "                    self._one_batch()\n",
    "        \n",
    "    def _one_batch(self):\n",
    "        self.xb, self.yb = self.batch\n",
    "        self.predict()\n",
    "        self.get_loss()\n",
    "        if self.model.training:\n",
    "            self.backward()\n",
    "            self.step()\n",
    "            self.scheduler_step()\n",
    "            self.zero_grad()\n",
    "            \n",
    "    def callback(self, name): \n",
    "        if self.cbs is not None:\n",
    "            for cb in sorted(self.cbs, key=attrgetter('order')): \n",
    "                method = getattr(cb, name, None)\n",
    "                if method is not None: method()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef70efc1-5a97-4660-9083-3625e2c6e5f6",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d6b9e4-24d4-47f7-9767-b7327cc6aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Callback(): \n",
    "    \"\"\"\n",
    "        Base callback class establishing that callbacks can have an order.\n",
    "        Callbacks inherit from this class and optionally update the order \n",
    "        parameter, to enable sequential ordering of callback functions that \n",
    "        depend on each other.\n",
    "    \"\"\"\n",
    "    order = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34febf6e-170b-4410-a92a-0c55065bc519",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec8731-f922-4df7-a8ac-2ecd4784b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def to_cpu(b):\n",
    "    \"\"\"\n",
    "        Returns data to the CPU.\n",
    "    \"\"\"\n",
    "    if isinstance(b, list): return [to_cpu(o) for o in b]\n",
    "    if isinstance(b, tuple): return tuple(to_cpu(list(b)))\n",
    "    return b.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a40f86e-c41c-499b-93c9-49f50f9409e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MetricsCB(Callback):\n",
    "    \"\"\"\n",
    "        Establishes and calculates metrics for training, and prints them\n",
    "        out at the end of each epoch. Metrics include train loss, validation\n",
    "        loss and optional metrics from the `torcheval` library.\n",
    "    \"\"\"\n",
    "    def __init__(self, *ms, **metrics):\n",
    "        for o in ms: metrics[type(o).__name__] = o\n",
    "        self.metrics = metrics\n",
    "        self.all_metrics = metrics\n",
    "        self.all_metrics['loss'] = Mean()\n",
    "        \n",
    "    def _log(self): \n",
    "        print(self.log)\n",
    "    \n",
    "    def before_fit(self): self.learn.metrics = self\n",
    "    def before_full_epoch(self):\n",
    "        self.log = pd.DataFrame({\n",
    "            \"Train loss\": 0,\n",
    "            \"Valid loss\": 0,\n",
    "            \"Accuracy\": 0\n",
    "        }, index=range(self.learn.epoch, self.learn.epoch+1))\n",
    "    def before_epoch(self): [o.reset() for o in self.all_metrics.values()]\n",
    "    def after_batch(self):\n",
    "        x, y = to_cpu(self.learn.batch)\n",
    "        self.metrics['accuracy'].update(to_cpu(self.learn.preds), y)\n",
    "        self.metrics['loss'].update(to_cpu(self.learn.loss), weight=len(x))\n",
    "    def after_epoch(self): \n",
    "        if self.learn.model.training: self.log['Train loss'] = round(float(self.all_metrics['loss'].compute().detach()), 4)\n",
    "        if not self.learn.model.training: \n",
    "            self.log['Valid loss'] = round(float(self.all_metrics['loss'].compute().detach()), 4)\n",
    "            self.log['Accuracy'] = round(float(self.all_metrics['accuracy'].compute().detach()), 4)\n",
    "    def after_full_epoch(self):\n",
    "        # log = {k:f\"{v.compute():.3f}\" for k, v in self.all_metrics.items()}\n",
    "        self._log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac15c2-931c-4592-b448-92ff9730d26a",
   "metadata": {},
   "source": [
    "### Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2c76a3-223a-4ad4-8cbd-fd59ecc9a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ProgressCB(Callback):\n",
    "    \"\"\"\n",
    "        Handles progress bars during training, and an optional plot parameter \n",
    "        plots the change in loss across training steps.\n",
    "    \"\"\"\n",
    "    order = MetricsCB.order + 1\n",
    "    def __init__(\n",
    "        self, \n",
    "        plot=False # If true, plots the change in loss across training steps\n",
    "    ): \n",
    "        self.plot = plot\n",
    "        if plot: self.losses, self.counter = [], 0\n",
    "        \n",
    "    def before_fit(self): self.learn.epochs = master_bar(self.learn.epochs, total=self.learn.n_epochs)\n",
    "    \n",
    "    def before_epoch(self):\n",
    "        self.learn.dl = progress_bar(self.learn.dl, leave=False, total=len(self.learn.dl))\n",
    "    def after_batch(self):\n",
    "        if self.plot and self.learn.model.training:\n",
    "            self.losses.append(float(self.learn.loss.detach()))\n",
    "            self.counter += 1\n",
    "    \n",
    "    def after_fit(self):\n",
    "        if self.plot:\n",
    "            self._plot()\n",
    "            \n",
    "    def _plot(self):\n",
    "        fig, ax = plt.subplots(figsize=(4, 4))\n",
    "        ax.plot(range(self.counter), self.losses)\n",
    "        ax.set_title('Change in loss')\n",
    "        ax.set_xlabel('Steps')\n",
    "        ax.set_ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a612dc3-2d24-4efe-a312-c65da7b1713c",
   "metadata": {},
   "source": [
    "### Send to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644efe88-fb6e-4409-956f-cd80fe6b8d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "        Returns the available device in the current environment as\n",
    "        a string.\n",
    "    \"\"\"\n",
    "    if torch.backends.mps.is_available(): device = 'mps' \n",
    "    if torch.cuda.is_available(): device = 'cuda'\n",
    "    else: device = 'cpu'\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbb0261-1b04-4f88-9736-3cd6e6fbd0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DeviceCB(Callback):\n",
    "    \"\"\"\n",
    "        Sends both the model and batch data to the device.\n",
    "    \"\"\"\n",
    "    def __init__(self): self.device = get_device()\n",
    "    def before_fit(self): self.learn.model.to(self.device)\n",
    "    def before_batch(self): \n",
    "        xb, yb = self.learn.batch\n",
    "        self.learn.batch = (xb.to(self.device), yb.to(self.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13324321-ec16-4f79-9471-ff7eda7ef95f",
   "metadata": {},
   "source": [
    "### Subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35108123-ad68-4930-b0ec-d23404a65f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseLearner(Learner):\n",
    "    \"\"\"\n",
    "        Flexible training subclass that handles key training functionality\n",
    "        for each batch, and enables training functionality to be substituted. \n",
    "        Uses an optional scheduler for one cycle training if desired.\n",
    "    \"\"\"\n",
    "    def predict(self): self.preds = self.model(self.xb)\n",
    "    def get_loss(self): self.loss = self.loss_func(self.preds, self.yb)\n",
    "    def backward(self): self.loss.backward()\n",
    "    def step(self): self.opt.step()\n",
    "    def scheduler_step(self): \n",
    "        if self.one_cycle: self.scheduler.step()\n",
    "    def zero_grad(self): self.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dd37cd-f9ad-4b2d-84d0-e909999e1cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [MetricsCB(accuracy=MulticlassAccuracy()), ProgressCB(plot=True), DeviceCB()]\n",
    "model = get_model()\n",
    "learn = BaseLearner(dls, model, cbs=cbs)\n",
    "learn.fit(0.2, 5, one_cycle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ba4fb5-4d8e-4e05-9bcb-06be59a91444",
   "metadata": {},
   "source": [
    "## Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ebf92-3806-466e-a290-ead148c0b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MomentumLearner(BaseLearner):\n",
    "    \"\"\"\n",
    "        Training subclass which implements momentum in a memory-efficient\n",
    "        way. Gradient updates are calculated directly in the tensor gradient, \n",
    "        and thus avoids storing a history of gradient updates.\n",
    "    \"\"\"\n",
    "    def __init__(self, dls, model, opt_func=optim.SGD, loss_func=F.cross_entropy, cbs=None, mom=0.85): \n",
    "        self.mom = mom\n",
    "        super().__init__(dls, model, opt_func=opt_func, loss_func=loss_func, cbs=cbs)\n",
    "    def zero_grad(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.model.parameters(): p.grad *= self.mom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205339c0-46fb-41b2-ba63-c32eb98dfd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [MetricsCB(accuracy=MulticlassAccuracy()), ProgressCB(plot=True), DeviceCB()]\n",
    "model = get_model()\n",
    "learn = MomentumLearner(dls, model, cbs=cbs)\n",
    "learn.fit(0.2, 5, one_cycle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942d70b1-e028-408e-80d8-7c041bc184ce",
   "metadata": {},
   "source": [
    "## LR Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef57ff2f-32c2-4a9e-9bdf-3e99cc1615b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361748b9-3b14-4d59-a429-277f3dd7ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LRFinderCB(Callback):\n",
    "    \"\"\"\n",
    "        Finds a suitable learning rate for the training data, by\n",
    "        implementing Leslie Smith's learning rate finder algorithm. The\n",
    "        learning rate is increased exponentially by a scalar until the loss \n",
    "        skyrockets, and a graph of loss vs. learning rate is returned.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=1.3): self.gamma = gamma\n",
    "    def before_fit(self): \n",
    "        self.lrs, self.losses = [], []\n",
    "        self.min = math.inf\n",
    "        self.sched = ExponentialLR(self.learn.opt, self.gamma)\n",
    "        \n",
    "    def after_batch(self):\n",
    "        if not self.learn.model.training: raise CancelEpochException()\n",
    "        self.lrs.append(self.learn.opt.param_groups[0]['lr'])\n",
    "        loss = to_cpu(self.learn.loss)\n",
    "        self.losses.append(loss)\n",
    "        if loss < self.min: self.min = loss\n",
    "        if loss > self.min*3: \n",
    "            plt.plot(self.lrs, self.losses)\n",
    "            plt.xscale('log')\n",
    "            plt.xlabel('Learning Rate')\n",
    "            plt.ylabel('Loss')\n",
    "            raise CancelFitException()\n",
    "        self.sched.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0fd515-1dd4-4f83-9b51-79ea7d8333fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [DeviceCB(), MetricsCB(accuracy=MulticlassAccuracy()), ProgressCB(), LRFinderCB()]\n",
    "model = get_model()\n",
    "learn = MomentumLearner(dls, model, cbs=cbs)\n",
    "learn.fit(0.001, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea764423-2cfe-4894-82f8-43aff0054ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d23d287-c3bc-4d37-8747-1dfcf53b8bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
